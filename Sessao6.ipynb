{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPytAxobVCe9wsVHPINmLZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Podinis/Pyhton_10810/blob/main/Sessao6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introdução a modelos não supervisionados<br>\n",
        "Clusters <br>"
      ],
      "metadata": {
        "id": "0nT9o0FdVEfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8OtCul3lUGGG",
        "outputId": "ed0ef674-b540-4204-ee1d-76e87e4a0118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb0z_-Jp9tBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "# Mock data for testing\n",
        "mock_sentences = [\n",
        "    \"I love using the Transformers library!\",\n",
        "    \"Once upon a time, in a faraway land,\",\n",
        "    \"What is the capital of France?\"\n",
        "]\n",
        "\n",
        "mock_context = \"France is a country in Europe. Its capital is Paris.\"\n",
        "mock_question = \"What is the capital of France?\"\n",
        "\n",
        "\n",
        "# Sentiment Analysis\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "for sentence in mock_sentences[:1]:  # Testing only the first sentence for sentiment analysis\n",
        "    sentiment_result = sentiment_analyzer(sentence)\n",
        "    print(f\"Sentiment Analysis Result for '{sentence}': {sentiment_result}\")\n",
        "\n",
        "# Text Generation\n",
        "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "for sentence in mock_sentences[1:2]:  # Testing only the second sentence for text generation\n",
        "    text_generation_result = text_generator(sentence, max_length=50, num_return_sequences=1)\n",
        "    print(f\"Text Generation Result for '{sentence}': {text_generation_result}\")\n",
        "\n",
        "# Question Answering\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "qa_result = qa_pipeline({\n",
        "    \"question\": mock_question,\n",
        "    \"context\": mock_context\n",
        "})\n",
        "print(f\"Question Answering Result: {qa_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwsf-SKX4_hb",
        "outputId": "02f60f36-1d6f-45c4-fc4a-f56a779e61fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Result for 'I love using the Transformers library!': [{'label': 'POSITIVE', 'score': 0.9993904829025269}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Answering Result: {'score': 0.9841360449790955, 'start': 46, 'end': 51, 'answer': 'Paris'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Criar pipeline de geração de texto\n",
        "gerador = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Gerar texto com base num prompt\n",
        "resposta = gerador(\"Today I'm going to give a class on artificial intelligence and\", max_length=50, num_return_sequences=1)\n",
        "\n",
        "print(resposta[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2TNoHOK725w",
        "outputId": "8e7cc72f-6d39-466d-e9c7-8da62efb8577"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today I'm going to give a class on artificial intelligence and how it's going to impact the world. The point is that these machines just don't know how to code. So, they think, \"Well, they don't have all the information that we have now and they can't build machines that do that. So, we're going to take this information and turn it into machines that do that.\" And then what we're going to do is we're going to go do things that create machines that have information and then we're going to make them smarter, and then we're going to make them better.\n",
            "\n",
            "I remember watching a TED talk with the guys that were doing it and the guys that were doing it in other areas of computer science. One of them was, as I said, the guy who was the first person to write a machine that did that and what he called the \"computer's view\"—the view that computers are better at understanding and understanding information. And then he developed a new view of the world that he called \"supervised learning.\" And that was the view that computers are more intelligent than humans. And so, if you understand the world of what we're doing, you can understand the world of what's happening in the world that we're in. And what he called supervised learning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xPQmH2kzVDi5"
      }
    }
  ]
}